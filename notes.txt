Next steps:
    - Reproduce results from Tishby completely.
    - Vary the results one variable at the time (dataset, activation function, binning/Gaussian)

Advanced Kernel Gaussian approx: looks very similar to expectation maximisation 

-----------------------------------------------------------------------------------------------------------------------

- Each dot is 1 NN.
   - this only makes sense when we bin variables and we get collisions in the input variable X.
     If we do not bin variables we get no collisions which implies the following:
                       ∀x∈ X. (∃!y. P(x, y) = 1) ∧ (∀y. P(x,y) = 0 ∨ P(x, y) = 1), holds.
     That is there exist only 1 y for a given x

    (Q: is there anything fundamentally wrong with that ?) - maybe if every x is unique and every y is unique then the
function is reversible ⇒ mutual information between the two random variables is 0. (is this not contradicted by I* ??)


    I = ∑(x,y)P(x, y)log(P(x,y)/(P(x)P(y)))

    If there exists a unique y for every x then I becomes

    I* = ∑(x) 1 log(1/(P(x)P(y)), which might not be a very good way to measure MI.

- binning / Gaussian approx
    - if we use Gaussian we get a normal function for every x. Our X distribution between binning method and Kernel
      Gaussian method remains the same (I think double check)

    Questions : 
        - Gaussian is a continuous function how will I measure mutual information of it ?
        - do I need to apply Kernel Gaussian method on X and Y or only on Ti (I suspect on all of them, my guess is
          there is a good way to measure mutual information between normal distributions.)
- MI only makes sense because of binning

-----------------------------------------------------------------------------------------------------------------------

 w1 - w2 - w3 - w4 - w5

Hypothesis:
    first layers of NN are more random than the later ones. In order to change the later layers first layers need to be
fairly static otherwise the NN couldn't trust them (ex. suppose we first train the later layers, then if we change the
first layers the assumptions made by the later layers would be broken. Hence it makes sense to conclude that first
layers are changed more and are more random while the later layers are fairly static from epoch to epoch(or from
minibatch to minibatch depends of what an epoch actually means).


-----------------------------------------------------------------------------------------------------------------------

Ideal neural networks cannot lose information.

Since the nodes are a real we can have an encoding as follows:

Suppose the following network.

2 nodes in the first layers 1 node in the second layer
   0.a1a2a3a4..   o-\
                     |-o  0.a1b1a2b2a3b3..
   0.b1b2b3b4..   o-/        

        R2         ->           R

, our NN are not ideal they have essentially a binning a function applied with 2^32 bins (precision of float)








